CS130 Project Review
====================

Team performing review:  <your team name here>
Work being reviewed:  <name of team whose work is being reviewed>

The first two sections are for reviewing the `sheets` library code itself,
excluding the test code and other aspects of the project.  The remaining
sections are for those other supporting parts of the project.

Feedback comments on design aspects of the `sheets` library
-----------------------------------------------------------

Consider the overall design and structure of the `sheets` library from
the perspective of the GRASP principles (Lecture 20) - in particular the
principles of high cohesion and low coupling.  What areas of the project
codebase are structured in a highly effective way?  What areas of the
codebase could be restructured to have higher cohesion and/or lower
coupling?  Give specific suggestions for how to achieve this in the code.

The overall design and structure of the library is outstanding. Each module
created serves a highly specific purpose that achieves high cohesion through-
out the library. For example, the separation of formula evaluator into the
interp.py file and the parser in parser.py are parts of the code that could
reside within cell.py. However, doing so would make cell.py dense and lower
its cohesion. Another impressive example of excellent design choice is the
creation of utils.py. Although the file has low cohesion with several
miscellaneous functions, it lowers the coupling between modules that would
have occurred if those functions had to exist elsewhere. Thus, the utils.py
serves to lower coupling throughout the codebase even if it has low cohesion.

Feedback comments on implementation aspects of the `sheets` library
-------------------------------------------------------------------

Consider the actual implementation of the project from the perspectives
of coding style (naming, commenting, code formatting, decomposition into
functions, etc.), and idiomatic use of the Python language and language
features.  What practices are used effectively in the codebase to make
for concise, readable and maintainable code?  What practices could or
should be incorporated to improve the quality, expressiveness, readability
and maintainability of the code?

The function_dir was a confusing aspect to the codebase that took time to
understand why it existed as an argument to several functions. However, we
understand now that the purpose is allow for custom functions to be added
into a workbook. All workbooks start with the same dictionary of functions but
can have unique functions appended to their copy. This is a brilliant design
to allow for users to create custom functions, but there currently exists no
mention of this in the comments. We initially thought it would be simpler if
the interp.py accessed function_dir rather than have it passed down from
workbook to any instance of cell evaluation. However, we think the
extensibility aspect is great but should be commented.

A small improvement can be made with the use of error_map. It exists in both
cell.py and interp.py but are separate variables from each other. This can
be forgotten when more errors are added or changed resulting in differing
behaviors depending on if an error is in a formula or not. Thus, an error_map
constant should be created within error_types.py which both modules already
import. Both modules can then access the same dictionary and maintain
consistent behavior.

There is one small note about the function names in the lark file. The
function name cannot be started with an underscore.

Feedback comments on testing aspects of the project
---------------------------------------------------

Consider the testing aspects of the project, from the perspective of "testing
best practices" (Lectures 4-6):  completeness/thoroughness of testing,
automation of testing, focus on testing the "most valuable" functionality vs.
"trivial code," following the Arrange-Act-Assert pattern in individual tests,
etc.  What testing practices are employed effectively in the project?  What
testing practices should be incorporated to improve the quality-assurance
aspects of the project?

The tests are all thorough testing basic expectations of features and slowly
escalating into more nuanced uses of the feature. The tests all utilize
unittest and are ran using github workflow making automation easy. Tests are
even made for important components of code that will not be touched by users
such as test_graph which tests that cycle detection and topological sorting
for cell evaluation.

One concern about the tests is the performance testing code. The profiler is
enabled using the setUp function in unittest.TestCase which runs before every
test. This means that any arranging of the workbook is included in the profile
which can make it hard to determin hotspots of a specific function call.

In addition, the assertion of many of the tests occurs within a for-test_loop
which can make it bit difficult to determine which iteration of the loop was
responsible for the test failing. Although it will make the test code longer,
separating more of the tests into smaller ones will make test failures more
clear as to which line or cells are responsible for the failure.

Consider the implementation quality of the testing code itself, in the same
areas described in the previous section.  What practices are used effectively
in the testing code to make it concise, readable and maintainable?  What
practices could or should be incorporated to improve the quality of the
testing code?

The quality of the testing code is amazing clear test names and comments
throughout the code detailing which what each section of the test checks for.
A small improvement to the testing code is the use of unittest.TestCase's
feature of self.assertRaise() which can be used as a context object to run
code and catch desired exceptions or fail otherwise. For example, instead of

try:
    do_something()
    self.fail()
except CertainError:
    pass

it can be written as

with self.assertRaises(CertainError):
    do_something

This will help will making the code concise and clear in what it is testing.

Feedback comments on other aspects of the project
-------------------------------------------------

If you have any other comments - compliments or suggestions for improvement -
that aren't covered by previous sections, please include them here.

<your team's feedback here>
